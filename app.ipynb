{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: pinecone-client in ./.venv/lib/python3.11/site-packages (3.0.2)\n",
      "Requirement already satisfied: cohere in ./.venv/lib/python3.11/site-packages (4.45)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./.venv/lib/python3.11/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.11/site-packages (from openai) (2.6.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.venv/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in ./.venv/lib/python3.11/site-packages (from pinecone-client) (2023.11.17)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in ./.venv/lib/python3.11/site-packages (from pinecone-client) (2.2.0)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in ./.venv/lib/python3.11/site-packages (from cohere) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2.0,>=1.8 in ./.venv/lib/python3.11/site-packages (from cohere) (1.9.3)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in ./.venv/lib/python3.11/site-packages (from cohere) (6.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.11/site-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in ./.venv/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets openai pinecone-client cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x: {\n",
    "    \"id\": f'{x[\"id\"]}-{x[\"chunk-id\"]}',\n",
    "    \"text\": x[\"chunk\"],\n",
    "    \"metadata\": {\n",
    "        \"title\": x[\"title\"],\n",
    "        \"url\": x[\"source\"],\n",
    "        \"primary_category\": x[\"primary_category\"],\n",
    "        \"published\": x[\"published\"],\n",
    "        \"updated\": x[\"updated\"],\n",
    "        \"text\": x[\"chunk\"],\n",
    "    }\n",
    "})\n",
    "data = data.remove_columns([\n",
    "    \"journal_ref\", \"primary_category\", \"published\", \"updated\", \"references\", \"authors\", \"categories\", \"comment\", \"title\", \"summary\", \"source\", \"doi\", \"chunk-id\", \"chunk\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = os.getenv(\"OPENAI_API_KEY\", getpass.getpass(\"Enter your OpenAI API key: \"))\n",
    "\n",
    "openai.api_key = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "  api_key=openai_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(docs: list[str]) -> list[list[float]]:\n",
    "    res = client.embeddings.create(input=docs, model=embed_model)\n",
    "    embeddings = [e.embedding for e in res.data]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"PINECONE_API_KEY\", getpass.getpass(\"Enter your Pinecone API key: \"))\n",
    "\n",
    "pc = Pinecone(\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dimension': 1536,\n",
       "  'host': 'rerankers-r3rtzs8.svc.apw5-4e34-81fa.pinecone.io',\n",
       "  'metric': 'dotproduct',\n",
       "  'name': 'rerankers',\n",
       "  'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "  'status': {'ready': True, 'state': 'Ready'}}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes().indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = \"rerankers\"\n",
    "\n",
    "if not any(d['name'] == index_name for d in pc.list_indexes().indexes):\n",
    "\n",
    "    pc.create_index(\n",
    "        name=\"rerankers\",\n",
    "        dimension=1536,\n",
    "        metric=\"dotproduct\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-west-2'\n",
    "        ) \n",
    "    ) \n",
    "\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        print(\"Waiting for index to be ready...\")\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [11:42<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    passed = False\n",
    "    # find end of batch\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # create batch\n",
    "    batch = data[i:i_end]\n",
    "    embeds = embed(batch[\"text\"])\n",
    "    to_upsert = list(zip(batch[\"id\"], embeds, batch[\"metadata\"]))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(query: str, top_k: int) -> list[str]:\n",
    "    # encode query\n",
    "    xq = embed([query])[0]\n",
    "    # search pinecone index\n",
    "    res = index.query(vector=xq, top_k=top_k, include_metadata=True)\n",
    "    # get doc text\n",
    "    docs = {x[\"metadata\"]['text']: i for i, x in enumerate(res[\"matches\"])}\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whichmodels areprompted toexplain theirreasoningwhen givena complexproblem, inorder toincrease\n",
      "the likelihood that their ﬁnal answer is correct.\n",
      "RLHF has emerged as a powerful strategy for ﬁne-tuning Large Language Models, enabling signiﬁcant\n",
      "improvements in their performance (Christiano et al., 2017). The method, ﬁrst showcased by Stiennon et al.\n",
      "(2020) in the context of text-summarization tasks, has since been extended to a range of other applications.\n",
      "In this paradigm, models are ﬁne-tuned based on feedback from human users, thus iteratively aligning the\n",
      "models’ responses more closely with human expectations and preferences.\n",
      "Ouyang et al. (2022) demonstrates that a combination of instruction ﬁne-tuning and RLHF can help ﬁx\n",
      "issues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai\n",
      "et al. (2022b) partially automates this ﬁne-tuning-plus-RLHF approach by replacing the human-labeled\n",
      "ﬁne-tuningdatawiththemodel’sownself-critiquesandrevisions,andbyreplacinghumanraterswitha\n",
      "---\n",
      "We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\n",
      "increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\n",
      "these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\n",
      "previous work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\n",
      "personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\n",
      "to control for the amount of RLHF training in the analysis of our experiments.\n",
      "3.2 Experiments\n",
      "3.2.1 Overview\n",
      "We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\n",
      "and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\n",
      "harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n",
      "[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\n",
      "decisions about individuals based on protected characteristics that should have no relevance to the outcome.5\n",
      "To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n",
      "---\n",
      "model to estimate the eventual performance of a larger RL policy. The slopes of these lines also\n",
      "explain how RLHF training can produce such large effective gains in model size, and for example it\n",
      "explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.\n",
      "• One can ask a subtle, perhaps ill-deﬁned question about RLHF training – is it teaching the model\n",
      "new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We\n",
      "might attempt to make this distinction sharp by associating the latter class of behaviors with the\n",
      "region where RL reward remains linear inp\n",
      "KL.\n",
      "• To make some bolder guesses – perhaps the linear relation actually provides an upper bound on RL\n",
      "reward, as a function of the KL. One might also attempt to extend the relation further by replacingp\n",
      "KLwith a geodesic length in the Fisher geometry.\n",
      "By making RL learning more predictable and by identifying new quantitative categories of behavior, we\n",
      "might hope to detect unexpected behaviors emerging during RL training.\n",
      "4.4 Tension Between Helpfulness and Harmlessness in RLHF Training\n",
      "Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we\n",
      "found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
      "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they\n",
      "---\n",
      "preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
      "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
      "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
      "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
      "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
      "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
      "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
      "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
      "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
      "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
      "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
      "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n",
      "---\n",
      "unnecessary. More details about RL are provided in B.1.\n",
      "Throughout this paper we use rPM=the preference model score itself for the RL reward. Recall that as\n",
      "implied by equation (2.1), this means that the difference in rPMvalues between two samples AandBwill\n",
      "be related to the predicted probability P(A>B )thatAwill be preferred to Bvia\n",
      "P(A>B ) =1\n",
      "1 +erPM(B)\u0000rPM(A)(4.2)\n",
      "There is no good reason13to use this preference model score directly as the reward, but it has been used in\n",
      "prior work such as [Stiennon et al., 2020] and so for simplicity we will not explore variations on this choice\n",
      "here.\n",
      "In order to produce additional prompts (i.e. the human side of the conversations) for RLHF training, we used\n",
      "a large LM to generate them. For this purpose, we simply used few-shot learning, creating a context with\n",
      "about 10 existing high-quality human queries, and then sampling to generate more. We ﬁnd that the sample\n",
      "efﬁciency of RLHF is roughly the same on the original crowdworker-written prompt dataset and the modelgenerated one, so we combine the two for greater diversity during RLHF training. We used 137k prompts\n",
      "---\n",
      "found that RLHF model biases are very strongly correlated with the bias of the underlying language models.\n",
      "That said, further work will be required to understand if this is a limitation of RLHF as a technique, or of\n",
      "our particular HH datasets. In any case, we likely need to build more subtle and comprehensive evaluations\n",
      "that include multi-turn dialogue, as this is an area where humans will likely use the models, and it’s also a\n",
      "place where it’s inherently more difﬁcult to measure performance against subtle objectives such as bias and\n",
      "fairness.\n",
      "On a much more practical level, we do not have much experience applying RL techniques to large generative\n",
      "models. Experienced AI practitioners know that there are a large variety of tweaks and tricks that require\n",
      "experimentation to identify, and that can majorly improve the stability and performance of training. We have\n",
      "18To be clear, we mean truly, thoroughly, and fundamentally, and not ‘merely behaviorally’ in some limited contexts.\n",
      "35\n",
      "encountered some stability issues with RL, and although we performed some rudimentary hyperparameter\n",
      "scans, we expect that with more experience and study we could do better. We also did not explore variations\n",
      "in online training, such as literally updating a single PM or RLHF model; rather we retrained these models\n",
      "---\n",
      "logic here is that if a model can really ‘helpfully follow instructions’, then a prompt or explanation should\n",
      "be sufﬁcient to bridge the zero-to-few-shot gap. We are very far from achieving this level of performance!\n",
      "Even on the honesty evaluation TruthfulQA [Lin et al., 2021] we close a bit less than half of this gap (Figure\n",
      "5). We also brieﬂy investigated whether our RLHF-ﬁnetuned code models have any comparative advantage\n",
      "when exposed to prompts including buggy code [Chen et al., 2021], but we did not ﬁnd any beneﬁts there.\n",
      "One would hope a fully aligned model would do its best to write correct code, even when given a buggy\n",
      "prompt.\n",
      "We also harbor a general concern that perhaps our techniques only render models aligned ‘on the surface’,\n",
      "and that they still harbor harmful biases or other tendencies that may surface in more subtle contexts. We\n",
      "found that RLHF models have a more positive sentiment towards all racial and religious groups, which seems\n",
      "promising, but does not necessarily indicate that biases have been reduced. And with respect to gender, we\n",
      "found that RLHF model biases are very strongly correlated with the bias of the underlying language models.\n",
      "---\n",
      "the context, and model-written functions are run in a sandbox environment. In Figure 21 we show results\n",
      "versus model size with and without RLHF training. We see the same trend here as with other evaluations –\n",
      "RLHF decreases the performance of small models, but improves the performance of larger models.\n",
      "RL training tends to decrease the entropy of the models’ distribution, and so we were concerned that these\n",
      "results would be very sensitive to temperature and top-p tuning. So for our 52B models, we performed a\n",
      "scan over temperatures and two top-p settings for both the RLHF models and the base code models, and then\n",
      "chose the best setting for each model and pass@k . We did a grid-search over the evaluation hyperparameters:\n",
      "T2f0;0:4;0:6;0:8;1:0g\u0002p2f0:95;1g\u0002k2f1;5;10;25;50;75;100g. Results are summarized on the\n",
      "right side of Figure 21. For each model and for each kinpass@k , we take the maximum performance over\n",
      "all 10 combinations of hyperparameters. We see that RLHF improves performance over the baseline on this\n",
      "evaluation, for all pass@k .\n",
      "We should emphasize that as with our other evaluations, the improvements in performance from RLHF are\n",
      "---\n",
      "response. We made this choice so that we could fully explore the vulnerability of our models to red-teaming.\n",
      "However, from the point of view of RLHF this was problematic, because beyond the ﬁrst turn of dialogue,\n",
      "our models never learned what a sophisticated response to a harmful query might be like. Our dataset does\n",
      "not provide guidance on the upper end of the distribution, on what models should do, but only tells models\n",
      "what notto do.\n",
      "In practice, we have partially resolved the optimization issue by training on a larger fraction of helpfulness\n",
      "prompts during RLHF. But in the future we hope to more fully and systematically address this problem by\n",
      "collecting harmlessness data where crowdworkers choose the best possible response from our models.14In\n",
      "this way we hope that rather than simply shutting down harmful requests, models can learn the more subtle\n",
      "art of ‘hostage negotiation’ with red-teamers.\n",
      "Note that since the data and models discussed in this section are from an earlier stage of our research, the RL\n",
      "results may look slightly different from other parts of the paper.\n",
      "4.5 Iterated Online RLHF\n",
      "In preceding sections we discussed the problem that PMs become progressively less calibrated and less robust\n",
      "at higher scores, as seen in the PM calibration study in Figure 9, and the RLHF robustness study in Figure\n",
      "---\n",
      "TriviaQA. On zero-shot tasks, RLHF training for helpfulness and harmlessness hurts performance for small\n",
      "models, but actually improves performance for larger models. Full results for each task are given in Figure\n",
      "28 (zero-shot) and Figure 29 (few-shot).\n",
      "Alignment with Human Values Has Many Beneﬁts and Essentially No Cost to Performance\n",
      "• Smaller models experience severe ‘alignment taxes’ – their performance on a wide variety of evaluations declines after RLHF training. However, we ﬁnd a variety of alignment bonuses , with our\n",
      "13B and 52B5RLHF-trained models performing better at zero-shot NLP evaluations, and the same\n",
      "at few-shot evaluations.\n",
      "• Natural language RLHF training for HH can be applied to models that have been ﬁrst ﬁnetuned\n",
      "on code, and it improves their programming ability on evaluations (presumably by improving\n",
      "general-purpose instruction following). We also ﬁnd that mixing preference model training for HH\n",
      "with the specialized skill of summarization [Stiennon et al., 2020] incurs no degradation in performance in either HH or summarization. So there is no reason not to combine alignment training with\n",
      "more speciﬁc, valuable skills.\n",
      "• There is a tension between helpfulness and harmlessness , which can be measured at the level of\n",
      "---\n",
      "signiﬁcant room for improvement. Note that our instructions to crowdworkers suggest that ‘lying isn’t helpful’ and that they should choose responses that are ‘helpful and honest’, so this is presumably related to the\n",
      "improvements we see on TruthfulQA. That said, we do not currently expect RLHF to be the best approach to\n",
      "honesty.\n",
      "16One possible caveat, however, is that our human feedback data was collected with 52B models, so perhaps the fact\n",
      "that the data is on-distribution for these models was relevant here.\n",
      "22\n",
      "Figure 17 Here we show sentiment scores (higher is more favorable sentiment) for samples generated from\n",
      "various prompts involving races and religions. We see that the predominant effect of RLHF training is to\n",
      "improve sentiment towards all groups.\n",
      "Another set of questions involves the underlying biases of these models. We evaluate our models for sentiment\n",
      "biases on race and religion (in the same format as Gopher [Rae et al., 2021]), for gender bias, and on the Bias\n",
      "Benchmark for QA (BBQ-lite) [Parrish et al., 2021].\n",
      "Results for sentiment towards different racial and religious groups are shown in Figure 17. The main effect\n",
      "we observe is that the sentiment of our RLHF-trained models tends to be much more positive than that of\n",
      "---\n",
      "in online training, such as literally updating a single PM or RLHF model; rather we retrained these models\n",
      "from scratch on each iteration. Another direction for exploration is to use a non-trivial function of PM scores\n",
      "as the RL reward, distorting the score distribution to e.g. focus more on discouraging bad behavior rather\n",
      "than rewarding good behavior. In summary, there are many future directions to explore for improving RLHF.\n",
      "A ﬁnal concern is whether techniques like those we have employed will continue to apply as AI models\n",
      "become increasingly capable. We take these concerns very seriously. In our view, the present work makes\n",
      "some progress towards our initial goal, which is to establish a set of simple and universal techniques19that\n",
      "can align AI models at present capability levels. Assuming this goal can be met, one of the next steps will be\n",
      "to build consensus among researchers and to understand alignment in greater depth, including how techniques\n",
      "scale with AI capabilities. The hope will be to create an evolving pragmatic state of the art for training AIs\n",
      "that are thoroughly helpful, honest, and harmless.\n",
      "Another essential step will be to use this baseline as a point of departure for exploring other techniques that\n",
      "can better-address more advanced use cases and more speculative failure modes. New ideas and techniques\n",
      "can then be pragmatically compared with existing methods, and then incorporated into standard practice if\n",
      "---\n",
      "Results show that RLHF actually improves performance, even at large k.\n",
      "38. Appendix B.8 further describes the format of the prompts we used (i.e., ‘HHH prompts’), which consist\n",
      "of a couple of code examples.\n",
      "We also conducted experiments involving adding buggy code to the prompts, which typically worsens performance (see [Chen et al., 2021]). We found that RLHF models did not perform better than their initial base\n",
      "code model snapshots, when these prompts are included in the context during evaluation, even after scanning\n",
      "over temperature and top-p.\n",
      "5.4 Applying Out-of-Distribution Detection to Reject Strange or Harmful Requests\n",
      "In this work we are primarily focused on achieving harmlessness entirely through natural language dialogue.\n",
      "However, one might try to avoid harmful behavior in a somewhat different manner, by either restricting\n",
      "language assistants to only respond to a narrow range of queries (approved-list), or by ﬁltering and rejecting\n",
      "known types of bad behavior (block-list). We could use our preference models for these purposes, but we\n",
      "might also take a different, less supervised approach, and leverage advances in out-of-distribution (OOD)\n",
      "detection. Such an approach might also be useful for those who want to build systems that only respond to a\n",
      "narrow range of queries (e.g. code models that should avoid non-code topics).\n",
      "---\n",
      "drawn based on them. In essence, we appear to ﬁnd that our plain LMs do not exhibit very signiﬁcant biases\n",
      "on this benchmark, using the available metrics. Here the metric is roughly ‘how often does the model invoke\n",
      "a term in a negative context, divided by how often it uses the term generally’. A value of 0.5 corresponds\n",
      "to the models being ‘unbiased’. RLHF training perhaps improves the results slightly (likely by improving\n",
      "sentiment) though the effect is very small (see Figure 18). We may return to these evaluations in the future\n",
      "using the full BBQA metrics [Parrish et al., 2021].\n",
      "In Section 3.4.2 we investigated gender bias in preference models. We tried using this format for evaluating\n",
      "RLHF models, but we found that these models were more than 10\u0002less likely than plain or context-distilled\n",
      "LMs to use a gendered term to complete sentences like ‘The CEO was a’. In other words, statements like\n",
      "‘The CEO was a father’ seem to be somewhat off-distribution for the RLHF models, at least compared to\n",
      "general LMs. This is problematic for evaluations, because it means that the bias measurements are likely\n",
      "to be dominated by noise, and are unlikely to be representative of model behavior (i.e. model-generated\n",
      "---\n",
      "the model outputs safe responses, they are often more detailed than what the average annotator writes.\n",
      "Therefore, after gathering only a few thousand supervised demonstrations, we switched entirely to RLHF to\n",
      "teachthemodelhowtowritemorenuancedresponses. ComprehensivetuningwithRLHFhastheadded\n",
      "beneﬁt that it may make the model more robust to jailbreak attempts (Bai et al., 2022a).\n",
      "WeconductRLHFbyﬁrstcollectinghumanpreferencedataforsafetysimilartoSection3.2.2: annotators\n",
      "writeapromptthattheybelievecanelicitunsafebehavior,andthencomparemultiplemodelresponsesto\n",
      "theprompts,selectingtheresponsethatissafestaccordingtoasetofguidelines. Wethenusethehuman\n",
      "preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to\n",
      "sample from the model during the RLHF stage.\n",
      "BetterLong-TailSafetyRobustnesswithoutHurtingHelpfulness Safetyisinherentlyalong-tailproblem,\n",
      "wherethe challengecomesfrom asmallnumber ofveryspeciﬁc cases. Weinvestigatetheimpact ofSafety\n",
      "---\n",
      "Discrimination in AdmissionsFigure 2 Inﬂuence of RLHF training (x-axes) for metrics for metrics for stereotype bias or discrimination (y-axes) for\n",
      "the 175B parameter model. (Left) Bias score for the BBQ benchmark in the ambiguous context across all categories\n",
      "(y-axis). Increasing the amount of RLHF steps decreases bias across all conditions, with the strongest decrease in the\n",
      "Q+IF condition (orange). (Middle) Correlation coefﬁcient \u001abetween the probability that models use female gendered\n",
      "pronouns coreferent with an occupation, p\u0012(female ), and corresponding estimate of fraction women in that occupation\n",
      "from the U.S. Bureau of Labor Statistics, pBLS(female )(y-axis). RLHF training does not signiﬁcantly inﬂuence \u001ain any\n",
      "condition. (Right) Difference between the probability a model thinks a student should be admitted to a class when their\n",
      "race is Black versus white, all else equal (y-axis). RLHF training decreases discrimination in the Q condition (blue) but\n",
      "is not enough to achieve demographic parity (dashed line). RLHF training achieves demographic parity at \u0018600 steps in\n",
      "the Q+IF (orange) condition and discriminates against white students with further RLHF steps. We see a similar trend for\n",
      "---\n",
      "In a certain sense, work on reinforcement learning from human feedback [Stiennon et al., 2020,\n",
      "Bai et al., 2022, Ouyang et al., 2022] has already taken a step in the direction of scaled supervision, since\n",
      "the reward signal in RL actually comes from an AI preference model (PM) rather than from immediate human oversight. However, RLHF typically uses tens of thousands of human preference labels.\n",
      "Here, we will test methods that reduce human input to an extreme, in order to study their viability. We will\n",
      "ﬁnetune AI models to be harmless using only of order ten2simple principles, stated in natural language.\n",
      "2These principles were chosen in a fairly ad hoc and iterative way for research purposes. In the future, we believe\n",
      "such principles should be redeveloped and reﬁned by a larger set of stakeholders, and that they should also be adapted\n",
      "depending on the intended usage and location in which the model may be deployed. Since such a small number of bits of\n",
      "information are involved in these principles, it’s worth studying these bits carefully.\n",
      "3\n",
      "101051010\n",
      "Parameters250\n",
      "200\n",
      "150\n",
      "100\n",
      "50\n",
      "050100150Helpfulness Elo\n",
      "SL-CAI\n",
      "Helpful RLHF\n",
      "HH RLHF\n",
      "RL-CAI\n",
      "RL-CAI w/ CoT\n",
      "101051010\n",
      "Parameters200\n",
      "150\n",
      "100\n",
      "50\n",
      "---\n",
      "distribution and aligns towards the human preference. This phenomena is illustrated in Figure 20, where we\n",
      "can see that the worst answers are progressively removed, shifting the distribution to the right.\n",
      "In addition, during annotation, the model has the potential to venture into writing trajectories that even the\n",
      "bestannotatorsmaynotchart. Nonetheless,humanscanstillprovidevaluablefeedbackwhencomparingtwo\n",
      "answers, beyond their own writing competencies. Drawing a parallel, while we may not all be accomplished\n",
      "artists, our ability to appreciate and critique art remains intact. We posit that the superior writing abilities of\n",
      "LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF, as\n",
      "documented in Gilardi et al. (2023) and Huang et al. (2023). Supervised data may no longer be the gold\n",
      "standard, and this evolving circumstance compels a re-evaluation of the concept of “supervision.”\n",
      "In-Context Temperature Rescaling. We have observed an intriguing phenomenon related to RLHF, a\n",
      "featurenotpreviouslyreportedtothebestofourknowledge: thedynamicre-scalingoftemperaturecontingent\n",
      "uponthecontext. AsindicatedinFigure8,thetemperatureappearstobeinﬂuencedbyRLHF.Yet,intriguingly,\n",
      "---\n",
      "fail to make a perfectly safe systems. Figure 10 shows examples of harmful outputs from the RS and RLHF\n",
      "models, respectively. For the RS case, the model at ﬁrst responds to a harmful inquiry, then starts to demur as\n",
      "the the conversation turns more harmful. For the RLHF case, we see a similar pattern, however the assistant\n",
      "remains helpful (though fabricates information) before ultimately refusing to help the human.\n",
      "To further understand the landscape of possible harms surfaced using this approach, across all model sizes and\n",
      "interventions, we created and annotated a visualization of the entire dataset (Figure 2). To do so, we obtained\n",
      "the average per token embeddings of each transcript from the residual stream in the 48th layer of the 52B\n",
      "prompted LM. Then we used UMAP [38] to turn the high-dimensional embeddings into two-dimensional\n",
      "embeddings for visualization. Intuitively, we expect this procedure to place any pair of transcripts closer\n",
      "together in this two dimensional space the more semantically similar to each other they are.\n",
      "We ﬁnd evidence for basic clusters of red team attempts. These include perhaps more obvious types of\n",
      "attacks, such as those soliciting discriminatory or offensive responses but also some surprising attacks. For\n",
      "example, we found a small cluster of attacks that tried to solicit misinformation in clever and subtle ways, and\n",
      "---\n",
      "ModelHuman-F eedback\n",
      "Comparison\n",
      "DataFigure 2 This diagram summarizes our data collection and model training workﬂow.\n",
      "helpful can lead to responses that help humans cause harm or generate toxic content. We demonstrate this\n",
      "tension quantitatively by showing that preference models trained to primarily evaluate one of these qualities\n",
      "perform very poorly (much worse than chance) on the other. Fortunately, we ﬁnd that PMs trained on a\n",
      "mixture of both datasets can nevertheless learn the right lessons and behave helpfully when appropriate,\n",
      "while encouraging the polite refusal of harmful requests. With preference models in hand, we then train\n",
      "helpful and harmless assistants via reinforcement learning, using the PM scores as rewards. We evaluate both\n",
      "PM performance and the more relevant performance characteristics of our RLHF-trained models. As can\n",
      "be seen in Figure 1, purely helpful RLHF-trained models are far easier to red-team, while helpful+harmless\n",
      "models are both very helpful and much less harmful.\n",
      "A question that’s often raised about alignment training is whether it will compromise AI capabilities. We\n",
      "ﬁnd that when RLHF is applied to large language models, the answer seems to be an almost-categorical\n",
      "no. Our RLHF-trained models tend to perform better than their raw, generative counterparts on virtually all\n",
      "---\n",
      "honesty, and harmlessness. We compare the performance of a preference model, trained on human feedback\n",
      "data, to pretrained language models, which evaluate the comparisons as multiple choice questions. We see\n",
      "that chain of thought reasoning signiﬁcantly improves the performance at this task. The trends suggest that\n",
      "models larger than 52B will be competitive with human feedback-trained preference models.\n",
      "1.4 Models and Data\n",
      "We use a series of language models, pretrained in the way we described in prior work [Bai et al., 2022].\n",
      "As our goal is to train helpful and harmless assistants from purely helpful assistants, we use RLHF to train\n",
      "our initial helpful models. For this we use the same process, but using only helpfulness human feedback\n",
      "(HF) data. However, as a point of comparison, we have also trained new preference models and helpful and\n",
      "harmless RLHF policies using human feedback.\n",
      "In our prior work [Bai et al., 2022], we collected human feedback data for preference model comparisons.\n",
      "Speciﬁcally, each data sample consists of a prompt and a pair of model-generated responses to the prompt; a\n",
      "crowdworker then labels the response deemed more helpful or harmless, depending on the task at hand. The\n",
      "helpfulness and harmlessness data are collected separately, and workers are asked to ‘red team’ the model\n",
      "---\n",
      "We have shown that it’s possible to use reinforcement learning from human feedback to train language models\n",
      "that act as helpful and harmless assistants. Our RLHF training also improves honesty, though we expect\n",
      "other techniques can do better still. As in other recent works associated with aligning large language models\n",
      "[Stiennon et al., 2020, Thoppilan et al., 2022, Ouyang et al., 2022, Nakano et al., 2021, Menick et al., 2022],\n",
      "RLHF improves helpfulness and harmlessness by a huge margin when compared to simply scaling models\n",
      "up.\n",
      "Our alignment interventions actually enhance the capabilities of large models, and can easily be combined\n",
      "with training for specialized skills (such as coding or summarization) without any degradation in alignment\n",
      "or performance. Models with less than about 10B parameters behave differently, paying an ‘alignment tax’ on\n",
      "their capabilities. This provides an example where models near the state-of-the-art may have been necessary\n",
      "to derive the right lessons from alignment research.\n",
      "The overall picture we seem to ﬁnd – that large models can learn a wide variety of skills, including alignment, in a mutually compatible way – does not seem very surprising. Behaving in an aligned fashion is just\n",
      "another capability, and many works have shown that larger models are more capable [Kaplan et al., 2020,\n",
      "---\n",
      "feedback model (typically a pretrained LM). Once the desired comparison labels are obtained, the remainder\n",
      "of the training pipeline (i.e., preference model training and RL) is exactly the same as RLHF.\n",
      "We begin by presenting the assistant model with a prompt, and generating a pair of responses. We then\n",
      "present the prompt and response pair to the feedback model with a principle for choosing the more harmless\n",
      "response, in a format like\n",
      "Consider the following conversation between a human and an assistant:\n",
      "[HUMAN/ASSISTANT CONVERSATION]\n",
      "[PRINCIPLE FOR MULTIPLE CHOICE EVALUATION]\n",
      "Options:\n",
      "(A) [RESPONSE A]\n",
      "(B) [RESPONSE B]\n",
      "The answer is:\n",
      "We then compute the log probability of the responses (A) and(B), and we make a labeled, preference\n",
      "modeling comparison example with the normalized probabilities as targets (and we expect these targets will\n",
      "be fairly well-calibrated [Kadavath et al., 2022], since they are multiple choice responses). We use pre-trained\n",
      "10\n",
      "models for feedback for the experiments in this section, but in Section 2 we also compare against helpful\n",
      "RLHF models in terms of label accuracy on various datasets.\n",
      "---\n",
      "an iterated online mode of training, where preference models and RL policies are updated\n",
      "on a weekly cadence with fresh human feedback data, efﬁciently improving our datasets\n",
      "and models. Finally, we investigate the robustness of RLHF training, and identify a roughly\n",
      "linear relation between the RL reward and the square root of the KL divergence between the\n",
      "policy and its initialization. Alongside our main results, we perform peripheral analyses on\n",
      "calibration, competing objectives, and the use of OOD detection, compare our models with\n",
      "human writers, and provide samples from our models using prompts appearing in recent\n",
      "related work.\n",
      "\u0003Correspondence to: {yuntao, jared}@anthropic.com\n",
      "Author contributions are listed at the end of the paper.arXiv:2204.05862v1  [cs.CL]  12 Apr 2022\n",
      "Contents\n",
      "1 Introduction 4\n",
      "1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "1.2 Summary of Evaluations and Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "---\n",
      "at higher scores, as seen in the PM calibration study in Figure 9, and the RLHF robustness study in Figure\n",
      "4. We believe this is caused by a lack of data in this high score regime. To address this, we propose iterated\n",
      "online RLHF :\n",
      "• We simply train the best RLHF policy we can, and use that to collect comparison data from crowdworkers. Since the policy was trained to optimize for PM score, it should produce responses that are\n",
      "on the upper end of the score distribution.\n",
      "• We mix the new comparison data with our existing data, and train a new scan of PMs, which we\n",
      "then use to train a new scan of RLHF policies. Then reiterate this process indeﬁnitely.\n",
      "Our hypothesis is that the ‘online’ RLHF policy helps us collect data on the upper end of the PM score\n",
      "distribution, which should improve PM calibration at high scores on subsequent iterations, and thereby allow\n",
      "us to train even better policies. Continuing this process should give us progressively better PMs and policies.\n",
      "Note that our use of the terminology ‘online’ is different from conventional use of the word—instead of\n",
      "training the same model iteratively, we retrain a new model per iteration.\n",
      "14In early versions of this experiment, we noticed that crowdworkers occasionally found it confusing to pick the least\n"
     ]
    }
   ],
   "source": [
    "query = \"can you explain why we would want to do rlhf?\"\n",
    "docs = get_docs(query, top_k=25)\n",
    "print(\"\\n---\\n\".join(docs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = os.getenv(\"COHERE_API_KEY\", getpass.getpass(\"Enter your Cohere API key: \"))\n",
    "\n",
    "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_docs = co.rerank(\n",
    "    query=query, documents=docs.keys(), top_n=25, model=\"rerank-english-v2.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'whichmodels areprompted toexplain theirreasoningwhen givena complexproblem, inorder toincrease\\nthe likelihood that their ﬁnal answer is correct.\\nRLHF has emerged as a powerful strategy for ﬁne-tuning Large Language Models, enabling signiﬁcant\\nimprovements in their performance (Christiano et al., 2017). The method, ﬁrst showcased by Stiennon et al.\\n(2020) in the context of text-summarization tasks, has since been extended to a range of other applications.\\nIn this paradigm, models are ﬁne-tuned based on feedback from human users, thus iteratively aligning the\\nmodels’ responses more closely with human expectations and preferences.\\nOuyang et al. (2022) demonstrates that a combination of instruction ﬁne-tuning and RLHF can help ﬁx\\nissues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai\\net al. (2022b) partially automates this ﬁne-tuning-plus-RLHF approach by replacing the human-labeled\\nﬁne-tuningdatawiththemodel’sownself-critiquesandrevisions,andbyreplacinghumanraterswitha'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_docs[0].document[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 21,\n",
       " 14,\n",
       " 3,\n",
       " 12,\n",
       " 9,\n",
       " 6,\n",
       " 8,\n",
       " 1,\n",
       " 15,\n",
       " 7,\n",
       " 20,\n",
       " 2,\n",
       " 17,\n",
       " 10,\n",
       " 19,\n",
       " 16,\n",
       " 24,\n",
       " 13,\n",
       " 23,\n",
       " 22,\n",
       " 4,\n",
       " 18,\n",
       " 11,\n",
       " 5]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[docs[doc.document[\"text\"]] for doc in rerank_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(query: str, top_k: int, top_n: int):\n",
    "    # first get vec search results\n",
    "    docs = get_docs(query, top_k=top_k)\n",
    "    i2doc = {docs[doc]: doc for doc in docs.keys()}\n",
    "    # rerank\n",
    "    rerank_docs = co.rerank(\n",
    "        query=query, documents=docs.keys(), top_n=top_n, model=\"rerank-english-v2.0\"\n",
    "    )\n",
    "    original_docs = []\n",
    "    reranked_docs = []\n",
    "    # compare order change\n",
    "    for i, doc in enumerate(rerank_docs):\n",
    "        rerank_i = docs[doc.document[\"text\"]]\n",
    "        print(str(i)+\"\\t->\\t\"+str(rerank_i))\n",
    "        if i != rerank_i:\n",
    "            reranked_docs.append(f\"[{rerank_i}]\\n\"+doc.document[\"text\"])\n",
    "            original_docs.append(f\"[{i}]\\n\"+i2doc[i])\n",
    "    for orig, rerank in zip(original_docs, reranked_docs):\n",
    "        print(\"ORIGINAL:\\n\"+orig+\"\\n\\nRERANKED:\\n\"+rerank+\"\\n\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t->\t0\n",
      "1\t->\t21\n",
      "2\t->\t14\n",
      "ORIGINAL:\n",
      "[1]\n",
      "We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\n",
      "increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\n",
      "these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\n",
      "previous work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\n",
      "personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\n",
      "to control for the amount of RLHF training in the analysis of our experiments.\n",
      "3.2 Experiments\n",
      "3.2.1 Overview\n",
      "We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\n",
      "and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\n",
      "harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n",
      "[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\n",
      "decisions about individuals based on protected characteristics that should have no relevance to the outcome.5\n",
      "To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n",
      "\n",
      "RERANKED:\n",
      "[21]\n",
      "We have shown that it’s possible to use reinforcement learning from human feedback to train language models\n",
      "that act as helpful and harmless assistants. Our RLHF training also improves honesty, though we expect\n",
      "other techniques can do better still. As in other recent works associated with aligning large language models\n",
      "[Stiennon et al., 2020, Thoppilan et al., 2022, Ouyang et al., 2022, Nakano et al., 2021, Menick et al., 2022],\n",
      "RLHF improves helpfulness and harmlessness by a huge margin when compared to simply scaling models\n",
      "up.\n",
      "Our alignment interventions actually enhance the capabilities of large models, and can easily be combined\n",
      "with training for specialized skills (such as coding or summarization) without any degradation in alignment\n",
      "or performance. Models with less than about 10B parameters behave differently, paying an ‘alignment tax’ on\n",
      "their capabilities. This provides an example where models near the state-of-the-art may have been necessary\n",
      "to derive the right lessons from alignment research.\n",
      "The overall picture we seem to ﬁnd – that large models can learn a wide variety of skills, including alignment, in a mutually compatible way – does not seem very surprising. Behaving in an aligned fashion is just\n",
      "another capability, and many works have shown that larger models are more capable [Kaplan et al., 2020,\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[2]\n",
      "model to estimate the eventual performance of a larger RL policy. The slopes of these lines also\n",
      "explain how RLHF training can produce such large effective gains in model size, and for example it\n",
      "explains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.\n",
      "• One can ask a subtle, perhaps ill-deﬁned question about RLHF training – is it teaching the model\n",
      "new skills or simply focusing the model on generating a sub-distribution of existing behaviors . We\n",
      "might attempt to make this distinction sharp by associating the latter class of behaviors with the\n",
      "region where RL reward remains linear inp\n",
      "KL.\n",
      "• To make some bolder guesses – perhaps the linear relation actually provides an upper bound on RL\n",
      "reward, as a function of the KL. One might also attempt to extend the relation further by replacingp\n",
      "KLwith a geodesic length in the Fisher geometry.\n",
      "By making RL learning more predictable and by identifying new quantitative categories of behavior, we\n",
      "might hope to detect unexpected behaviors emerging during RL training.\n",
      "4.4 Tension Between Helpfulness and Harmlessness in RLHF Training\n",
      "Here we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we\n",
      "found that many RLHF policies were very frequently reproducing the same exaggerated responses to all\n",
      "remotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they\n",
      "\n",
      "RERANKED:\n",
      "[14]\n",
      "the model outputs safe responses, they are often more detailed than what the average annotator writes.\n",
      "Therefore, after gathering only a few thousand supervised demonstrations, we switched entirely to RLHF to\n",
      "teachthemodelhowtowritemorenuancedresponses. ComprehensivetuningwithRLHFhastheadded\n",
      "beneﬁt that it may make the model more robust to jailbreak attempts (Bai et al., 2022a).\n",
      "WeconductRLHFbyﬁrstcollectinghumanpreferencedataforsafetysimilartoSection3.2.2: annotators\n",
      "writeapromptthattheybelievecanelicitunsafebehavior,andthencomparemultiplemodelresponsesto\n",
      "theprompts,selectingtheresponsethatissafestaccordingtoasetofguidelines. Wethenusethehuman\n",
      "preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to\n",
      "sample from the model during the RLHF stage.\n",
      "BetterLong-TailSafetyRobustnesswithoutHurtingHelpfulness Safetyisinherentlyalong-tailproblem,\n",
      "wherethe challengecomesfrom asmallnumber ofveryspeciﬁc cases. Weinvestigatetheimpact ofSafety\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(query, 25, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t->\t0\n",
      "1\t->\t4\n",
      "2\t->\t16\n",
      "ORIGINAL:\n",
      "[1]\n",
      "red-teaming expertise valuable for organizations with suf ﬁcient resources. However, it would also be\n",
      "beneﬁcial to experiment with the formation of a community of AI red teaming professionals that draws\n",
      "together individuals from different organizations and bac kgrounds, speciﬁcally focused on some subset\n",
      "of AI (versus AI in general) that is relatively well-deﬁned a nd relevant across multiple organizations.25\n",
      "A community of red teaming professionals could take actions such as publish best practices, collectively\n",
      "analyze particular case studies, organize workshops on eme rging issues, or advocate for policies that\n",
      "would enable red teaming to be more effective.\n",
      "Doing red teaming in a more collaborative fashion, as a commu nity of focused professionals across\n",
      "23Red teaming could be aimed at assessing various properties o f AI systems, though we focus on safety and security in this\n",
      "subsection given the expertise of the authors who contribut ed to it.\n",
      "24For an example of early efforts related to this, see Marshall et al., \"Threat Modeling AI /ML Systems and Dependencies\"\n",
      "[43]\n",
      "25In the context of language models, for example, 2019 saw a deg ree of communication and coordination across AI developers\n",
      "to assess the relative risks of different language understa nding and generation systems [10]. Adversarial machine learning,\n",
      "\n",
      "RERANKED:\n",
      "[4]\n",
      "by red teams allow organizations to improve security and sys tem integrity before and during deployment.\n",
      "Knowledge that a lab has a red team can potentially improve th e trustworthiness of an organization with\n",
      "respect to their safety and security claims, at least to the e xtent that effective red teaming practices exist\n",
      "and are demonstrably employed.\n",
      "As indicated by the number of cases in which AI systems cause o r threaten to cause harm, developers of an\n",
      "AI system often fail to anticipate the potential risks assoc iated with technical systems they develop. These\n",
      "risks include both inadvertent failures and deliberate mis use. Those not involved in the development\n",
      "of a particular system may be able to more easily adopt and pra ctice an attacker’s skillset. A growing\n",
      "number of industry labs have dedicated red teams, although b est practices for such efforts are generally\n",
      "in their early stages.24There is a need for experimentation both within and across or ganizations in order\n",
      "to move red teaming in AI forward, especially since few AI dev elopers have expertise in relevant areas\n",
      "such as threat modeling and adversarial machine learning [44].\n",
      "AI systems and infrastructure vary substantially in terms o f their properties and risks, making in-house\n",
      "red-teaming expertise valuable for organizations with suf ﬁcient resources. However, it would also be\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[2]\n",
      "more comprehensive way.\n",
      "We conducted a series of red teaming with various groups of internal employees, contract workers, and\n",
      "externalvendors. Theseteamsincludedover350people,includingdomainexpertsincybersecurity,election fraud, social media misinformation, legal, policy, civil rights, ethics, software engineering, machine\n",
      "learning, responsible AI, and creative writing. They also included individuals representative of a variety of\n",
      "socioeconomic, gender, ethnicity, and racial demographics.\n",
      "28\n",
      "Theredteamersprobedourmodelsacrossawiderangeofriskcategories(suchascriminalplanning,human\n",
      "traﬃcking, regulated or controlled substances, sexually explicit content, unqualiﬁed health or ﬁnancial\n",
      "advice, privacy violations, and more), as well as diﬀerent attack vectors (such as hypothetical questions,\n",
      "malformed/misspelledinputs,orextendeddialogues). Additionally,weconductedspeciﬁcteststodetermine\n",
      "the capabilities of our models to facilitate the production of weapons (e.g. nuclear, biological, chemical, and\n",
      "cyber); ﬁndingsonthesetopicsweremarginal andweremitigated. Nonetheless, wewill continueourred\n",
      "teaming eﬀorts in this front.\n",
      "\n",
      "RERANKED:\n",
      "[16]\n",
      "the training data [13], aiding in disinformation campaigns [12], generating extremist texts [37], spreading\n",
      "falsehoods [35], and more [9, 10, 18, 57, 22, 51]. As AI systems improve, the scope of possible harms seems\n",
      "likely to grow [22]. Many strategies have been developed to address some of these harms (e.g., [58, 4, 48,\n",
      "36, 34, 19, 60]). One potentially useful tool for addressing harm is red teaming—using manual or automated\n",
      "methods to adversarially probe a language model for harmful outputs, and then updating the model to avoid\n",
      "such outputs [42, 20, 3, 11]. In this paper, we describe our early efforts to implement manual red teaming to\n",
      "both make models safer and measure the safety of our models. The models trained with red team data were\n",
      "described in [4], so here we focus on describing our red team results and techniques in detail in the hope that\n",
      "others may beneﬁt from and improve on them.\n",
      "\u0003Correspondence to: {deep, liane, jackson, jared, jack}@anthropic.com\n",
      "Authors above the line break are core contributors. Author contributions are listed in §A.1.arXiv:2209.07858v2  [cs.CL]  22 Nov 2022\n",
      "2.7B 13B 52B\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(\"what is red teaming?\", top_k=25, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name) #saving resources bc pinecone bills u for active indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
